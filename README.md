**IMPORTANT : pour des raisons de confidentialit√© il faut ajouter le cours au format pdf sur le projet en local sur sa machine pour pouvoir utiliser ce notebook**

üéì Maths-LoRA-Tutor : Agent √âducatif Math√©matique sur Hardware Grand Public

Ce projet explore la cr√©ation d'un agent conversationnel sp√©cialis√© dans l'enseignement des math√©matiques de niveau ing√©nieur (Math√©matiques D√©terministes).L'objectif principal est de d√©montrer l'efficacit√© de la m√©thode LoRA (Low-Rank Adaptation) pour sp√©cialiser un "petit" mod√®le de langage (< 1 milliard de param√®tres) sur un domaine complexe, en utilisant uniquement des ressources de calcul grand public.

üìÑ Contexte & R√©f√©rence Th√©orique

Ce travail s'appuie sur le papier de recherche :LoRA: Low-Rank Adaptation of Large Language Models > Edward J. Hu et al. (Microsoft), 2021 - arXiv:2106.09685
Plut√¥t que de r√©entra√Æner tous les param√®tres du mod√®le (Full Fine-Tuning), LoRA g√®le les poids pr√©-entra√Æn√©s et injecte des matrices de rang faible entra√Ænables dans les couches d'attention. Cela permet de r√©duire drastiquement le co√ªt m√©moire et computationnel.

üõ†Ô∏è M√©thodologie

Le projet compare plusieurs approches impl√©ment√©es dans le notebook Agent √©ducatif avec LoRA.ipynb :
Pr√©paration des Donn√©es :
Extraction et nettoyage d'un polycopi√© de cours (PDF ‚Üí Txt).
G√©n√©ration synth√©tique de paires Questions/R√©ponses (Instruction Tuning) via un LLM tiers.
Fine-Tuning LoRA (Deux strat√©gies) :
Causal LM : Entra√Ænement sur le texte brut avec fen√™tre glissante pour apprendre le "style" et le LaTeX.
Instruct Tuning : Entra√Ænement sur le dataset de questions/r√©ponses pour apprendre le format "Flashcard".Config : Rang $r=8$, Alpha=16/32, Cible q_proj, v_proj.

Comparaison avec RAG (Retrieval Augmented Generation) :
Impl√©mentation d'un pipeline RAG avec FAISS et Cross-Encoder pour fournir le contexte au mod√®le de base.

üìä R√©sultats Cl√©s

L'√©valuation a √©t√© r√©alis√©e sur un jeu de test ind√©pendant en utilisant la similarit√© s√©mantique (Cosine Similarity) et le score ROUGE.

ApprochePerplexit√© (Test)
Score S√©mantique Moyen
ObservationMod√®le de Base
27.11-Hallucinations fr√©quentes, r√©ponses vagues.RAG-0.21Pertinent sur la r√©cup√©ration, mais √©chec de la synth√®se par le mod√®le 0.5B.LoRA (Instruct )5.440.50Meilleure performance. Style concis, vocabulaire math√©matique pr√©cis ($L^p$, Sobolev).

Constat : Sur un tr√®s petit mod√®le (0.5B), le Fine-Tuning LoRA s'av√®re plus efficace que le RAG pour capturer le jargon et la syntaxe sp√©cifique, le mod√®le manquant de capacit√©s de raisonnement pour exploiter efficacement le contexte RAG.

üöÄ Installation & Usage

Cloner le repo :Bashgit clone https://github.com/VOTRE_USERNAME/Maths-LoRA-Tutor.git
cd Maths-LoRA-Tutor

Installer les d√©pendances :Bashpip install torch transformers peft datasets bitsandbytes accelerate
pip install langchain-community faiss-cpu sentence-transformers rouge-score

Lancer le Notebook :
Ouvrez Agent √©ducatif avec LoRA.ipynb dans Jupyter ou VS Code. Le code d√©tecte automatiquement l'acc√©l√©ration mat√©rielle (CUDA pour Nvidia, MPS pour Apple Silicon).üìÇ Structure du R√©pertoire

Agent √©ducatif avec LoRA.ipynb : Le c≈ìur du projet (Nettoyage, Entra√Ænement LoRA, RAG, √âvaluation).
**.pdf : Le document source que vous devrez ajouter pour l'entra√Ænement (Cours de math√©matiques).**
maths_flashcards.json : Le dataset d'instruction g√©n√©r√©.
qwen_instruct_math et model_qwen_trained_with_sw_r=8 : peuvent √™tre utilis√©s directement pour √©viter de refaire l'entra√Ænement avec la m√©thode LoRA

‚ö†Ô∏è Limitations

Taille du mod√®le : Qwen2.5-0.5B est tr√®s l√©ger. Bien qu'il apprenne parfaitement la forme (LaTeX, d√©finitions), il peut manquer de fond sur des raisonnements logiques complexes compar√© √† des mod√®les 7B+.
Hallucination : Comme tout LLM fine-tun√© sans RAG externe, le mod√®le peut halluciner des formules plausibles mais fausses.
